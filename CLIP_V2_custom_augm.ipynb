{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNJHJFRscWRLFhMS1i9cygC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"EvuhKBaOJlZV"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","!pip install transformers torch torchvision\n","from transformers import CLIPProcessor, CLIPModel\n","import torch\n","import torchvision.transforms as transforms\n","import os\n","import urllib\n","import json\n","from PIL import Image\n","from copy import deepcopy\n","import numpy as np\n","from torch.optim import Adam\n","from torch.nn import BatchNorm1d\n","from tqdm import tqdm\n","\n","# init the processor and model\n","model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\").to('cuda')\n","processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n","model.eval()  # Evaluation mode\n","\n","# dataset and labels paths\n","dataset_path = '/content/drive/MyDrive/imagenetv2-matched-frequency/imagenetv2-matched-frequency-format-val'\n","labels_path = '/content/drive/MyDrive/imagenet1000_clsidx_to_labels.txt'\n","\n","if os.path.exists(dataset_path):\n","    print(f\"Dataset folder {dataset_path} found\")\n","else:\n","    print(f\"Dataset folder {dataset_path} not found\")\n","\n","# read labels\n","with open(labels_path, 'r') as f:\n","    clsidx_to_labels = ast.literal_eval(f.read())\n","\n","# Download ImageNet labels\n","LABELS_URL = \"https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\"\n","response = urllib.request.urlopen(LABELS_URL)\n","imagenet_labels = json.loads(response.read().decode())\n","\n","# Generate CLIP-compatible text labels\n","def generate_clip_labels(imagenet_labels):\n","    return [f\"a photo of a {label}\" for label in imagenet_labels]\n","\n","clip_labels = generate_clip_labels(imagenet_labels)\n","text_inputs = processor(text=clip_labels, return_tensors=\"pt\", padding=True).to('cuda')\n","\n","with torch.no_grad():\n","    text_features = model.get_text_features(**text_inputs)\n","\n","# augmentations\n","augmentations = transforms.Compose([\n","    transforms.RandomResizedCrop(224),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n","])\n","\n","# apply augmentations\n","def apply_augmentations(image, num_augmentations):\n","    augmented_images = [augmentations(image) for _ in range(num_augmentations)]\n","    return torch.stack(augmented_images)\n","\n","# compute entropy loss\n","def entropy_loss(preds):\n","    p_log_p = preds * torch.log(preds + 1e-10)\n","    return -p_log_p.sum(dim=1).mean()\n","\n","# MEMO adaptation function\n","def memo_adaptation(model, image, num_augmentations=5, num_steps=1, lr=1e-5):\n","    model.eval()  # evaluation mode\n","\n","    # generate custom augmentations\n","    augmented_images = apply_augmentations(image, num_augmentations).to('cuda')\n","\n","    # optimizer\n","    optimizer = Adam(model.parameters(), lr=lr)\n","\n","    # batch normalization\n","    batch_norm = BatchNorm1d(512)#.to('cuda')\n","\n","    for step in range(num_steps):\n","        optimizer.zero_grad()\n","\n","        # compute predictions\n","        outputs = model.get_image_features(augmented_images)\n","        outputs = batch_norm(outputs)\n","\n","        # compute marginal distribution\n","        outputs = torch.softmax(outputs, dim=1)\n","        marginal_output = outputs.mean(dim=0, keepdim=True)\n","\n","        # compute entropy loss\n","        loss = entropy_loss(marginal_output)\n","        loss.backward()\n","        optimizer.step()\n","\n","    return model\n","\n","results = []\n","ground_truth_labels = []\n","\n","for folders_counter, class_folder in enumerate(os.listdir(dataset_path)):\n","    if folders_counter >= 9999:  # limit folders for faster testing\n","        break\n","\n","    class_folder_path = os.path.join(dataset_path, class_folder)\n","\n","    if not os.path.isdir(class_folder_path):\n","        continue\n","\n","    ground_truth_label_idx = int(class_folder)  # class index is the folder name\n","    ground_truth_label = clsidx_to_labels[ground_truth_label_idx]\n","    images_counter = 0\n","    for img_name in os.listdir(class_folder_path):\n","        if images_counter >= 5:  # limit the images for faster testing\n","          break\n","        images_counter += 1\n","        img_path = os.path.join(class_folder_path, img_name)\n","        image = Image.open(img_path).convert('RGB')\n","\n","        # apply MEMO\n","        adapted_model = memo_adaptation(model, image)\n","\n","        # evaluate the adapted model\n","        with torch.no_grad():\n","            inputs = processor(images=image, return_tensors=\"pt\").to('cuda')\n","            outputs = adapted_model.get_image_features(inputs['pixel_values'])\n","            similarities = torch.matmul(outputs, text_features.T)\n","            pred_label_idx = torch.argmax(similarities, dim=1).item()\n","            pred_label = imagenet_labels[pred_label_idx]\n","            results.append(pred_label)\n","            ground_truth_labels.append(ground_truth_label)\n","\n","        # reset the weights\n","        model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\").to('cuda')\n","\n","# compute accuracy\n","accuracy = np.mean([1 if pred == true else 0 for pred, true in zip(results, ground_truth_labels)])\n","print(f\"Accuracy: {accuracy * 100:.2f}%\")\n","\n"]}]}