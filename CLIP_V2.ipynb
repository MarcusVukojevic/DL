{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO/LC7XJBN1g6Ulsej3jXfe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n2paEqSObP2Y","executionInfo":{"status":"ok","timestamp":1720002659846,"user_tz":-120,"elapsed":2251,"user":{"displayName":"Mattias Trettel","userId":"11220684259335201165"}},"outputId":"ee2ee8ee-afe4-47c2-e32c-3cae3e0687f6","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# dataset and labels are in the drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install transformers torch torchvision\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XrPMa7qIb1p_","executionInfo":{"status":"ok","timestamp":1720001891729,"user_tz":-120,"elapsed":9059,"user":{"displayName":"Mattias Trettel","userId":"11220684259335201165"}},"outputId":"7112dea6-a805-4648-99ea-90ecb797114d","collapsed":true},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"]}]},{"cell_type":"code","source":["from transformers import CLIPProcessor, CLIPModel\n","import torch\n","\n","# Init model and processor\n","model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\").to('cuda')\n","processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n","model.eval()  # evaluation mode"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"6Ar0kA5Vd-yO","outputId":"8d29087a-ae7c-4393-b248-c8cd7b754669","executionInfo":{"status":"ok","timestamp":1720001894522,"user_tz":-120,"elapsed":2795,"user":{"displayName":"Mattias Trettel","userId":"11220684259335201165"}}},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["CLIPModel(\n","  (text_model): CLIPTextTransformer(\n","    (embeddings): CLIPTextEmbeddings(\n","      (token_embedding): Embedding(49408, 512)\n","      (position_embedding): Embedding(77, 512)\n","    )\n","    (encoder): CLIPEncoder(\n","      (layers): ModuleList(\n","        (0-11): 12 x CLIPEncoderLayer(\n","          (self_attn): CLIPAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (mlp): CLIPMLP(\n","            (activation_fn): QuickGELUActivation()\n","            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","          )\n","          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (vision_model): CLIPVisionTransformer(\n","    (embeddings): CLIPVisionEmbeddings(\n","      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n","      (position_embedding): Embedding(197, 768)\n","    )\n","    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (encoder): CLIPEncoder(\n","      (layers): ModuleList(\n","        (0-11): 12 x CLIPEncoderLayer(\n","          (self_attn): CLIPAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): CLIPMLP(\n","            (activation_fn): QuickGELUActivation()\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n","  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",")"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["import os\n","import ast\n","\n","# dataset folder path + check\n","dataset_path = '/content/drive/MyDrive/imagenetv2-matched-frequency/imagenetv2-matched-frequency-format-val'\n","if os.path.exists(dataset_path):\n","    print(f\"Dataset folder {dataset_path} found\")\n","else:\n","    print(f\"Dataset folder {dataset_path} not found\")\n","\n","\n","# dataset labels path\n","file_path = '/content/drive/MyDrive/imagenet1000_clsidx_to_labels.txt'\n","\n","\n","# reading and creating dictionary\n","with open(file_path, 'r') as f:\n","    file_content = f.read()\n","\n","clsidx_to_labels = ast.literal_eval(file_content)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jWNNmk9gegIi","executionInfo":{"status":"ok","timestamp":1720001894523,"user_tz":-120,"elapsed":3,"user":{"displayName":"Mattias Trettel","userId":"11220684259335201165"}},"outputId":"c34ea67e-b068-49ae-c848-ecddb164314c"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["La cartella /content/drive/MyDrive/imagenetv2-matched-frequency/imagenetv2-matched-frequency-format-val Ã¨ presente.\n"]}]},{"cell_type":"code","source":["import torchvision.transforms as transforms\n","\n","# standard augmentation of CLIP processor\n","def augmentations(image, processor):\n","    return processor(images=image, return_tensors=\"pt\")[\"pixel_values\"].squeeze()"],"metadata":{"id":"dWLK0fYMhjUI","executionInfo":{"status":"ok","timestamp":1720002002672,"user_tz":-120,"elapsed":291,"user":{"displayName":"Mattias Trettel","userId":"11220684259335201165"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["import torch.nn.functional as F\n","\n","def entropy_loss(preds):\n","    p_log_p = preds * torch.log(preds + 1e-10)\n","    return -p_log_p.sum(dim=1).mean()\n"],"metadata":{"id":"0_You2q1hlYI","executionInfo":{"status":"ok","timestamp":1720002003889,"user_tz":-120,"elapsed":2,"user":{"displayName":"Mattias Trettel","userId":"11220684259335201165"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["from torch.optim import Adam\n","from torch.nn import BatchNorm1d\n","from PIL import Image\n","\n","\n","def memo_adaptation(model, processor, image, num_augmentations=5, lr=1e-5):\n","    model.train()\n","    optimizer = Adam(model.parameters(), lr=lr)\n","\n","    # augmentations generation and batch processing\n","    augmented_images = [augmentations(image, processor) for _ in range(num_augmentations)]\n","    augmented_images = torch.stack(augmented_images).to('cuda')\n","\n","    batch_norm = BatchNorm1d(512).to('cuda')\n","    optimizer.zero_grad()\n","\n","    # compute predictions\n","    outputs = model.get_image_features(augmented_images)\n","    outputs = batch_norm(outputs)\n","    print(outputs.shape)\n","\n","    # compute marginal distributions\n","    outputs = F.softmax(outputs, dim=1)\n","    marginal_output = outputs.mean(dim=0, keepdim=True)\n","\n","    # compute entropy loss\n","    loss = entropy_loss(marginal_output)\n","    loss.backward()\n","    optimizer.step()\n","\n","    return model"],"metadata":{"id":"KGxcTKhqhp8Q","executionInfo":{"status":"ok","timestamp":1720002655811,"user_tz":-120,"elapsed":287,"user":{"displayName":"Mattias Trettel","userId":"11220684259335201165"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["import os\n","import numpy as np\n","\n","results = []\n","ground_truth_labels = []\n","\n","# cycle on dataset folders\n","for folder_counter, class_folder in enumerate(os.listdir(dataset_path)):\n","    if folder_counter >= 1:  # to limit the number of folders\n","        break\n","\n","    class_folder_path = os.path.join(dataset_path, class_folder)\n","    if not os.path.isdir(class_folder_path):\n","        continue\n","\n","    ground_truth_label_idx = int(class_folder)  # the name of the folders is the class name (see labels file)\n","    ground_truth_label = clsidx_to_labels[ground_truth_label_idx]\n","\n","    image_counter = 0\n","    for img_name in os.listdir(class_folder_path):\n","        if image_counter >= 1:  # to limit the number of images\n","          break\n","        image_counter+=1\n","        img_path = os.path.join(class_folder_path, img_name)\n","        image = Image.open(img_path).convert('RGB')\n","\n","        # MEMO\n","        adapted_model = memo_adaptation(model, processor, image)\n","\n","        # new evaluation\n","        with torch.no_grad():\n","            inputs = processor(images=image, return_tensors=\"pt\").to('cuda')\n","            outputs = adapted_model.get_image_features(inputs['pixel_values'])\n","            pred_label_idx = torch.argmax(outputs, dim=1).item()\n","            pred_label = clsidx_to_labels[pred_label_idx]\n","            results.append(pred_label)\n","            ground_truth_labels.append(ground_truth_label)\n","\n","        # reset of the weights\n","        model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\").to('cuda')\n","\n","# accuracy\n","accuracy = np.mean([1 if pred == true else 0 for pred, true in zip(results, ground_truth_labels)])\n","print(f\"Accuracy: {accuracy * 100:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mDXKtRYGhxO5","executionInfo":{"status":"ok","timestamp":1720002410901,"user_tz":-120,"elapsed":16320,"user":{"displayName":"Mattias Trettel","userId":"11220684259335201165"}},"outputId":"a77d82b3-f6c0-44e2-8bac-914610570576","collapsed":true},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([5, 512])\n","Accuracy: 0.00%\n"]}]}]}