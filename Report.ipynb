{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Report Deep Learning Project** \n",
    "##### \n",
    "Marcus Vukojevik 238817 marcus.vukojevik@studenti.unitn.it  \n",
    "Mattias Trettel 247187 mattias.trettel@studenti.unitn.it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Project Description**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our approach was to leverage MEMO alongside BLIP as a complementary model to adjust class distributions manually prior to applying the entropy minimization algorithm. BLIP (Bootstrapping Language-Image Pretraining) is a vision-language model that generates textual descriptions or answers based on visual input and is particularly valuable for augmenting image classification tasks by providing additional semantic context. We used Imagenet-A as the dataset. The basic idea is simple: if a model has difficulty categorizing certain classes of images, why not get help from a second model? This second model can be used as a second opinion, given the predictions of the first model. In our case, we use BLIP to manually adjust the logit values (which are the unnormalized predictions of the classes) before applying entropy minimization. We query BLIP about the presence of ResNet's output labels in image augmentations, and if BLIP confirms their presence, we manually increase the probability of those classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Index**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Section 1**: \n",
    "- a) import of the dataset Imagenet-A and related class labels\n",
    "- b) defining argmix functions\n",
    "- c) defining augmentations functions\n",
    "- d) defining utilities function\n",
    "#### **Section 2**: **MEMO** baseline  \n",
    "#### **Section 3**: **MEMO** plus **BLIP**  \n",
    "\n",
    "###### \n",
    "Note: **Section 2** and **Section 3** both require the dataset, labels and functions imported and defined in **Section 1**. However, they can be executed independently of each other.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 1-a** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This cell is responsible for retrieving the ImageNet-A dataset and its class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import urllib\n",
    "import json\n",
    "\n",
    "# Dataset URL\n",
    "dataset_url = 'https://people.eecs.berkeley.edu/~hendrycks/imagenet-a.tar'\n",
    "\n",
    "# Destination path to download the file\n",
    "destination = '/content/imagenet-a.tar'\n",
    "\n",
    "# Download the file\n",
    "!wget -O {destination} {dataset_url}\n",
    "\n",
    "# Extract the .tar file (print suppressed)\n",
    "!tar -xvf {destination} -C /content/ > /dev/null 2>&1\n",
    "\n",
    "dataset_path = '/content/imagenet-a'\n",
    "\n",
    "# Verify extracted files\n",
    "num_folders = 0\n",
    "num_files = 0\n",
    "\n",
    "# Iterate over all items in the directory\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    num_folders += len(dirs)\n",
    "    num_files += len(files)\n",
    "\n",
    "print(\"=====================================\")\n",
    "print(f\"Number of folders: {num_folders}/200\")\n",
    "print(f\"Number of files: {num_files}/7501\")\n",
    "\n",
    "\n",
    "# Download the ImageNet class labels file\n",
    "LABELS_URL = \"https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\"\n",
    "try:\n",
    "    response = urllib.request.urlopen(LABELS_URL)\n",
    "    data = response.read().decode()\n",
    "    class_idx = json.loads(data)\n",
    "\n",
    "    # Check if `class_idx` is a list and contains elements\n",
    "    if isinstance(class_idx, list) and len(class_idx) > 0:\n",
    "        print(\"Download and loading of the labels completed\")\n",
    "    else:\n",
    "        print(\"Error: Wrong data format\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 1-b Augmix** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we define the **augmix** functions  \n",
    "AugMix is ​​a data augmentation technique designed to improve the robustness and generalization of machine learning models. The provided code implements the AugMix augmentation pipeline, which mixes multiple image transformations together to broaden the diversity of the training data while maintaining the semantic content of the images.\n",
    "#### Key steps:\n",
    "- **Preaugmentation**:  \n",
    "The original image undergoes a series of transformations. This ensures that the image is prepared in a way that is conducive to further augmentation.\n",
    "\n",
    "- **Augmentation Mixing**:  \n",
    "The code applies a set of predefined augmentations to the image and combines them using a weighted average. This process is controlled by a mixing coefficient derived from a Dirichlet distribution, which determines how much influence each augmentation will have in the final mixed image.\n",
    "\n",
    "- **Mixing of Augmented Images**:  \n",
    "The final image is created by mixing the preaugmented image with the mixed augmented images. A Beta distribution is used to decide the blending ratio, ensuring that the augmented images are combined in a diverse yet controlled manner.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from PIL import ImageOps, Image\n",
    "from torchvision import transforms\n",
    "\n",
    "## AugMix Data Augmentation\n",
    "## Reference: https://github.com/google-research/augmix\n",
    "\n",
    "def _augmix_aug(x_orig):\n",
    "    \"\"\"\n",
    "    Apply AugMix augmentation to the original image.\n",
    "    \n",
    "    Args:\n",
    "    - x_orig (PIL.Image): The original image to be augmented.\n",
    "    \n",
    "    Returns:\n",
    "    - mix (torch.Tensor): The augmented image tensor after mixing.\n",
    "    \"\"\"\n",
    "    # Apply pre-augmentation transformations\n",
    "    x_orig = preaugment(x_orig)\n",
    "    # Preprocess the original image\n",
    "    x_processed = preprocess(x_orig)\n",
    "    \n",
    "    # Sample weights for each augmentation\n",
    "    w = np.float32(np.random.dirichlet([1.0, 1.0, 1.0]))\n",
    "    # Sample mixing coefficient\n",
    "    m = np.float32(np.random.beta(1.0, 1.0))\n",
    "    \n",
    "    # Initialize an empty tensor to accumulate the mixed augmented images\n",
    "    mix = torch.zeros_like(x_processed)\n",
    "    \n",
    "    # Apply augmentations and mix images\n",
    "    for i in range(3):\n",
    "        x_aug = x_orig.copy()\n",
    "        # Apply a random number of augmentations\n",
    "        for _ in range(np.random.randint(1, 4)):\n",
    "            x_aug = np.random.choice(augmentations)(x_aug)\n",
    "        # Add weighted augmented image to mix\n",
    "        mix += w[i] * preprocess(x_aug)\n",
    "    \n",
    "    # Mix original and augmented images\n",
    "    mix = m * x_processed + (1 - m) * mix\n",
    "    return mix\n",
    "\n",
    "# Short alias for the augmentation function\n",
    "aug = _augmix_aug\n",
    "\n",
    "# Define various augmentation functions\n",
    "def autocontrast(pil_img, level=None):\n",
    "    \"\"\"Apply autocontrast to the image.\"\"\"\n",
    "    return ImageOps.autocontrast(pil_img)\n",
    "\n",
    "def equalize(pil_img, level=None):\n",
    "    \"\"\"Apply histogram equalization to the image.\"\"\"\n",
    "    return ImageOps.equalize(pil_img)\n",
    "\n",
    "def rotate(pil_img, level):\n",
    "    \"\"\"Rotate the image by a random angle.\"\"\"\n",
    "    degrees = int_parameter(rand_lvl(level), 30)\n",
    "    if np.random.uniform() > 0.5:\n",
    "        degrees = -degrees\n",
    "    return pil_img.rotate(degrees, resample=Image.BILINEAR, fillcolor=128)\n",
    "\n",
    "def solarize(pil_img, level):\n",
    "    \"\"\"Solarize the image by inverting colors above a certain threshold.\"\"\"\n",
    "    level = int_parameter(rand_lvl(level), 256)\n",
    "    return ImageOps.solarize(pil_img, 256 - level)\n",
    "\n",
    "def shear_x(pil_img, level):\n",
    "    \"\"\"Apply horizontal shear to the image.\"\"\"\n",
    "    level = float_parameter(rand_lvl(level), 0.3)\n",
    "    if np.random.uniform() > 0.5:\n",
    "        level = -level\n",
    "    return pil_img.transform((224, 224), Image.AFFINE, (1, level, 0, 0, 1, 0), resample=Image.BILINEAR, fillcolor=128)\n",
    "\n",
    "def shear_y(pil_img, level):\n",
    "    \"\"\"Apply vertical shear to the image.\"\"\"\n",
    "    level = float_parameter(rand_lvl(level), 0.3)\n",
    "    if np.random.uniform() > 0.5:\n",
    "        level = -level\n",
    "    return pil_img.transform((224, 224), Image.AFFINE, (1, 0, 0, level, 1, 0), resample=Image.BILINEAR, fillcolor=128)\n",
    "\n",
    "def translate_x(pil_img, level):\n",
    "    \"\"\"Translate the image horizontally.\"\"\"\n",
    "    level = int_parameter(rand_lvl(level), 224 / 3)\n",
    "    if np.random.random() > 0.5:\n",
    "        level = -level\n",
    "    return pil_img.transform((224, 224), Image.AFFINE, (1, 0, level, 0, 1, 0), resample=Image.BILINEAR, fillcolor=128)\n",
    "\n",
    "def translate_y(pil_img, level):\n",
    "    \"\"\"Translate the image vertically.\"\"\"\n",
    "    level = int_parameter(rand_lvl(level), 224 / 3)\n",
    "    if np.random.random() > 0.5:\n",
    "        level = -level\n",
    "    return pil_img.transform((224, 224), Image.AFFINE, (1, 0, 0, 0, 1, level), resample=Image.BILINEAR, fillcolor=128)\n",
    "\n",
    "def posterize(pil_img, level):\n",
    "    \"\"\"Reduce the number of bits for each color channel.\"\"\"\n",
    "    level = int_parameter(rand_lvl(level), 4)\n",
    "    return ImageOps.posterize(pil_img, 4 - level)\n",
    "\n",
    "# Helper functions to scale parameter values\n",
    "def int_parameter(level, maxval):\n",
    "    \"\"\"\n",
    "    Scale an integer parameter according to the level.\n",
    "    \n",
    "    Args:\n",
    "    - level (float): Level of the operation between [0, PARAMETER_MAX].\n",
    "    - maxval (int): Maximum value for the operation.\n",
    "    \n",
    "    Returns:\n",
    "    - int: Scaled integer parameter.\n",
    "    \"\"\"\n",
    "    return int(level * maxval / 10)\n",
    "\n",
    "def float_parameter(level, maxval):\n",
    "    \"\"\"\n",
    "    Scale a float parameter according to the level.\n",
    "    \n",
    "    Args:\n",
    "    - level (float): Level of the operation between [0, PARAMETER_MAX].\n",
    "    - maxval (float): Maximum value for the operation.\n",
    "    \n",
    "    Returns:\n",
    "    - float: Scaled float parameter.\n",
    "    \"\"\"\n",
    "    return float(level) * maxval / 10.\n",
    "\n",
    "def rand_lvl(n):\n",
    "    \"\"\"\n",
    "    Generate a random level for augmentation.\n",
    "    \n",
    "    Args:\n",
    "    - n (float): Maximum value for the random level.\n",
    "    \n",
    "    Returns:\n",
    "    - float: Random level between 0.1 and n.\n",
    "    \"\"\"\n",
    "    return np.random.uniform(low=0.1, high=n)\n",
    "\n",
    "# List of augmentations\n",
    "augmentations = [\n",
    "    autocontrast,\n",
    "    equalize,\n",
    "    lambda x: rotate(x, 1),\n",
    "    lambda x: solarize(x, 1),\n",
    "    lambda x: shear_x(x, 1),\n",
    "    lambda x: shear_y(x, 1),\n",
    "    lambda x: translate_x(x, 1),\n",
    "    lambda x: translate_y(x, 1),\n",
    "    lambda x: posterize(x, 1),\n",
    "]\n",
    "\n",
    "# Define preprocessing and pre-augmentation pipelines\n",
    "mean = [0.485, 0.456, 0.406]  # Mean for normalization\n",
    "std = [0.229, 0.224, 0.225]   # Standard deviation for normalization\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "preaugment = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 1-c ImageAugmentor** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a class called ImageAugmentor which is designed to apply various data augmentation techniques to an image and display the augmented images. It relies on argmix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import v2\n",
    "from PIL import ImageEnhance, ImageFilter\n",
    "import concurrent.futures\n",
    "import random\n",
    "\n",
    "#libraries already called:\n",
    "#from PIL import Image\n",
    "#from torchvision import transforms\n",
    "\n",
    "# Define a class for image augmentation\n",
    "class ImageAugmentor:\n",
    "    # Initialization method\n",
    "    def __init__(self, augmentations=None):\n",
    "        # If no augmentations are provided, use default set of augmentations\n",
    "        if augmentations is None:\n",
    "            self.augmentations = v2.Compose([\n",
    "                v2.RandomHorizontalFlip(),\n",
    "                v2.RandomVerticalFlip(),\n",
    "                v2.RandomRotation(degrees=45),\n",
    "                v2.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0)),\n",
    "            ])\n",
    "        else:\n",
    "            # If augmentations are provided, use those\n",
    "            self.augmentations = augmentations\n",
    "\n",
    "    # Method to apply augmentations to an image\n",
    "    def apply_augmentations(self, image_path, aug, n_augmentations):\n",
    "        # Load and convert the image to RGB\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        # Define a helper function to perform augmentation\n",
    "        def augment_image(_):\n",
    "            if aug:\n",
    "                return transforms.ToPILImage()(_augmix_aug(image))\n",
    "            else:\n",
    "                return self.augmentations(image)\n",
    "        \n",
    "        # Use ThreadPoolExecutor to perform augmentations in parallel\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            augmented_images = list(executor.map(augment_image, range(n_augmentations)))\n",
    "    \n",
    "        return augmented_images\n",
    "\n",
    "    # Method to apply a set of predefined augmentations to an image\n",
    "    def apply_all_augmentations(self, image_path):\n",
    "        # Load and convert the image to RGB\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        augmentations = []\n",
    "\n",
    "        # Helper function to apply an augmentation function with its arguments\n",
    "        def apply_augmentation(func, *args):\n",
    "            return func(*args)\n",
    "\n",
    "        # List of augmentation tasks with corresponding functions and arguments\n",
    "        tasks = [\n",
    "            (random_rotation, image, 360),\n",
    "            (random_crop, image, (int(image.width * 0.8), int(image.height * 0.8))),\n",
    "            (random_zoom, image, 0.8, 1.2),\n",
    "            (random_shift, image, 10, 10),\n",
    "            (shear_image, image, 0.2),\n",
    "            (adjust_brightness, image, 1.5),\n",
    "            (adjust_contrast, image, 1.5),\n",
    "            (adjust_saturation, image, 1.5),\n",
    "            (adjust_hue, image, 50),\n",
    "            (add_noise, image, 25),\n",
    "            (blur_image, image, 2),\n",
    "            (sharpen_image, image, 2),\n",
    "            (grayscale_image, image),\n",
    "            (cutout_image, image, 50),\n",
    "            (flip_image_horizontal, image),\n",
    "            (flip_image_vertical, image),\n",
    "            (rotate_image, image, 45),\n",
    "            (crop_image, image, (10, 10, image.width-10, image.height-10)),\n",
    "            (zoom_image, image, 1.1),\n",
    "            (shift_image, image, 5, 5),\n",
    "        ]\n",
    "\n",
    "        # Use ThreadPoolExecutor to perform all augmentations in parallel\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            results = [executor.submit(apply_augmentation, task[0], *task[1:]) for task in tasks]\n",
    "            for future in concurrent.futures.as_completed(results):\n",
    "                augmentations.append(future.result())\n",
    "\n",
    "        return augmentations\n",
    "\n",
    "    # Method to display the original and augmented images\n",
    "    def show_images(self, original_image, augmented_images):\n",
    "        # Create a subplot with the original and augmented images\n",
    "        fig, axes = plt.subplots(1, len(augmented_images) + 1, figsize=(15, 5))\n",
    "        axes[0].imshow(original_image)\n",
    "        axes[0].set_title(\"Original Image\")\n",
    "        axes[0].axis(\"off\")\n",
    "        \n",
    "        try:\n",
    "            # Display augmented images, converting to PIL if necessary\n",
    "            for i, aug_image in enumerate(augmented_images):\n",
    "                axes[i + 1].imshow(transforms.ToPILImage()(aug_image))\n",
    "                axes[i + 1].set_title(f\"Augmented Image {i+1}\")\n",
    "                axes[i + 1].axis(\"off\")\n",
    "        except:\n",
    "            for i, aug_image in enumerate(augmented_images):\n",
    "                axes[i + 1].imshow(aug_image)\n",
    "                axes[i + 1].set_title(f\"Augmented Image {i+1}\")\n",
    "                axes[i + 1].axis(\"off\")\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    # Augmentations functions\n",
    "def rotate_image(image, angle):\n",
    "    return image.rotate(angle)\n",
    "\n",
    "def crop_image(image, crop_area):\n",
    "    return image.crop(crop_area)\n",
    "\n",
    "def zoom_image(image, zoom_factor):\n",
    "    width, height = image.size\n",
    "    x_center, y_center = width / 2, height / 2\n",
    "    new_width, new_height = width / zoom_factor, height / zoom_factor\n",
    "    left = x_center - new_width / 2\n",
    "    top = y_center - new_height / 2\n",
    "    right = x_center + new_width / 2\n",
    "    bottom = y_center + new_height / 2\n",
    "    return image.crop((left, top, right, bottom)).resize((width, height), Image.LANCZOS)\n",
    "\n",
    "def shift_image(image, dx, dy):\n",
    "    width, height = image.size\n",
    "    return Image.fromarray(np.roll(np.roll(np.array(image), dx, axis=1), dy, axis=0), 'RGB')\n",
    "\n",
    "def flip_image_horizontal(image):\n",
    "    return image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "def flip_image_vertical(image):\n",
    "    return image.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "\n",
    "def random_rotation(image, max_angle):\n",
    "    angle = random.uniform(-max_angle, max_angle)\n",
    "    return rotate_image(image, angle)\n",
    "\n",
    "def random_crop(image, crop_size):\n",
    "    width, height = image.size\n",
    "    left = random.randint(0, width - crop_size[0])\n",
    "    top = random.randint(0, height - crop_size[1])\n",
    "    right = left + crop_size[0]\n",
    "    bottom = top + crop_size[1]\n",
    "    return crop_image(image, (left, top, right, bottom))\n",
    "\n",
    "def random_zoom(image, min_zoom, max_zoom):\n",
    "    zoom_factor = random.uniform(min_zoom, max_zoom)\n",
    "    return zoom_image(image, zoom_factor)\n",
    "\n",
    "def random_shift(image, max_dx, max_dy):\n",
    "    dx = random.randint(-max_dx, max_dx)\n",
    "    dy = random.randint(-max_dy, max_dy)\n",
    "    return shift_image(image, dx, dy)\n",
    "\n",
    "def shear_image(image, shear_factor):\n",
    "    width, height = image.size\n",
    "    xshift = abs(shear_factor) * width\n",
    "    new_width = width + int(round(xshift))\n",
    "    return image.transform((new_width, height), Image.AFFINE,\n",
    "                           (1, shear_factor, -xshift if shear_factor > 0 else 0, 0, 1, 0), Image.BICUBIC)\n",
    "\n",
    "def adjust_brightness(image, factor):\n",
    "    enhancer = ImageEnhance.Brightness(image)\n",
    "    return enhancer.enhance(factor)\n",
    "\n",
    "def adjust_contrast(image, factor):\n",
    "    enhancer = ImageEnhance.Contrast(image)\n",
    "    return enhancer.enhance(factor)\n",
    "\n",
    "def adjust_saturation(image, factor):\n",
    "    enhancer = ImageEnhance.Color(image)\n",
    "    return enhancer.enhance(factor)\n",
    "\n",
    "def adjust_hue(image, factor):\n",
    "    image = np.array(image.convert('HSV'))\n",
    "    image[..., 0] = (image[..., 0].astype(int) + factor) % 256\n",
    "    return Image.fromarray(image, 'HSV').convert('RGB')\n",
    "\n",
    "def add_noise(image, noise_level):\n",
    "    image = np.array(image)\n",
    "    noise = np.random.normal(0, noise_level, image.shape)\n",
    "    noisy_image = np.clip(image + noise, 0, 255).astype(np.uint8)\n",
    "    return Image.fromarray(noisy_image, 'RGB')\n",
    "\n",
    "def blur_image(image, radius):\n",
    "    return image.filter(ImageFilter.GaussianBlur(radius))\n",
    "\n",
    "def sharpen_image(image, factor):\n",
    "    enhancer = ImageEnhance.Sharpness(image)\n",
    "    return enhancer.enhance(factor)\n",
    "\n",
    "def grayscale_image(image):\n",
    "    return ImageOps.grayscale(image).convert('RGB')\n",
    "\n",
    "def cutout_image(image, mask_size, mask_value=0):\n",
    "    image = np.array(image)\n",
    "    height, width = image.shape[:2]\n",
    "    y = np.random.randint(height)\n",
    "    x = np.random.randint(width)\n",
    "    y1 = np.clip(y - mask_size // 2, 0, height)\n",
    "    y2 = np.clip(y + mask_size // 2, 0, height)\n",
    "    x1 = np.clip(x - mask_size // 2, 0, width)\n",
    "    x2 = np.clip(x + mask_size // 2, 0, width)\n",
    "    image[y1:y2, x1:x2] = mask_value\n",
    "    return Image.fromarray(image, 'RGB')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 1-d Utilities functions**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code contains several utilities for preprocessing images and manipulating the results, calculate metrics like marginal entropy and KL divergence to evaluate the uncertainty of the model's predictions, and map the class IDs output by the model to readable labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for pre-processing images\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),                                                         #Resizes the image to 256 pixels on the shortest side\n",
    "    transforms.CenterCrop(224),                                                     #Performs a centered crop of 224x224 pixels\n",
    "    transforms.ToTensor(),                                                          #Converts the image to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),    #Normalizes the image using the specified mean and standard deviation\n",
    "])\n",
    "\n",
    "# Function to load and pre-process an image\n",
    "def load_and_preprocess_image(img_path):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    img_tensor = preprocess(img)\n",
    "    img_tensor = img_tensor.unsqueeze(0)  # Aggiungi una dimensione per il batch\n",
    "    return img_tensor\n",
    "\n",
    "# Function to pre-process an already loaded image\n",
    "def process_image(img):\n",
    "    img_tensor = preprocess(img)\n",
    "    img_tensor = img_tensor.unsqueeze(0)  # Aggiungi una dimensione per il batch\n",
    "    return img_tensor\n",
    "\n",
    "\n",
    "# Function to calculate marginal entropy (an indication of the uncertainty in the model's predictions)\n",
    "def marginal_entropy(outputs):\n",
    "    logits = outputs - outputs.logsumexp(dim=-1, keepdim=True)\n",
    "    avg_logits = logits.logsumexp(dim=0) - np.log(logits.shape[0])\n",
    "    min_real = torch.finfo(avg_logits.dtype).min\n",
    "    avg_logits = torch.clamp(avg_logits, min=min_real)\n",
    "    return -(avg_logits * torch.exp(avg_logits)).sum(dim=-1), avg_logits\n",
    "\n",
    "# Function to calculate Kullback-Leibler (KL) divergence (how one probability distribution diverges from a reference distribution)\n",
    "def kl_div(outputs):\n",
    "    kl_divs = []\n",
    "    for i in range(outputs.size(1)):\n",
    "        class_probs = outputs[:, i]\n",
    "        class_avg_prob = class_probs.mean(dim=0)\n",
    "        kl_div = (class_probs * (torch.log(class_probs) - torch.log(class_avg_prob))).sum(dim=-1)\n",
    "        kl_divs.append(kl_div)\n",
    "    \n",
    "    # Sum of the KL Divergences\n",
    "    total_kl_div = torch.stack(kl_divs).sum()\n",
    "    return total_kl_div\n",
    "\n",
    "\n",
    "\n",
    "# Function to get the readable label from the class ID\n",
    "def get_class_label(class_id):\n",
    "    return class_mapping.get(class_id, \"Unknown class\")\n",
    "\n",
    "\n",
    "# ImageNet class mapping\n",
    "class_mapping_data = [\n",
    "    \"n01498041 stingray\",\n",
    "    \"n01531178 goldfinch\",\n",
    "    \"n01534433 junco\",\n",
    "    \"n01558993 American robin\",\n",
    "    \"n01580077 jay\",\n",
    "    \"n01614925 bald eagle\",\n",
    "    \"n01616318 vulture\",\n",
    "    \"n01631663 newt\",\n",
    "    \"n01641577 American bullfrog\",\n",
    "    \"n01669191 box turtle\",\n",
    "    \"n01677366 green iguana\",\n",
    "    \"n01687978 agama\",\n",
    "    \"n01694178 chameleon\",\n",
    "    \"n01698640 American alligator\",\n",
    "    \"n01735189 garter snake\",\n",
    "    \"n01770081 harvestman\",\n",
    "    \"n01770393 scorpion\",\n",
    "    \"n01774750 tarantula\",\n",
    "    \"n01784675 centipede\",\n",
    "    \"n01819313 sulphur-crested cockatoo\",\n",
    "    \"n01820546 lorikeet\",\n",
    "    \"n01833805 hummingbird\",\n",
    "    \"n01843383 toucan\",\n",
    "    \"n01847000 duck\",\n",
    "    \"n01855672 goose\",\n",
    "    \"n01882714 koala\",\n",
    "    \"n01910747 jellyfish\",\n",
    "    \"n01914609 sea anemone\",\n",
    "    \"n01924916 flatworm\",\n",
    "    \"n01944390 snail\",\n",
    "    \"n01985128 crayfish\",\n",
    "    \"n01986214 hermit crab\",\n",
    "    \"n02007558 flamingo\",\n",
    "    \"n02009912 great egret\",\n",
    "    \"n02037110 oystercatcher\",\n",
    "    \"n02051845 pelican\",\n",
    "    \"n02077923 sea lion\",\n",
    "    \"n02085620 Chihuahua\",\n",
    "    \"n02099601 Golden Retriever\",\n",
    "    \"n02106550 Rottweiler\",\n",
    "    \"n02106662 German Shepherd Dog\",\n",
    "    \"n02110958 pug\",\n",
    "    \"n02119022 red fox\",\n",
    "    \"n02123394 Persian cat\",\n",
    "    \"n02127052 lynx\",\n",
    "    \"n02129165 lion\",\n",
    "    \"n02133161 American black bear\",\n",
    "    \"n02137549 mongoose\",\n",
    "    \"n02165456 ladybug\",\n",
    "    \"n02174001 rhinoceros beetle\",\n",
    "    \"n02177972 weevil\",\n",
    "    \"n02190166 fly\",\n",
    "    \"n02206856 bee\",\n",
    "    \"n02219486 ant\",\n",
    "    \"n02226429 grasshopper\",\n",
    "    \"n02231487 stick insect\",\n",
    "    \"n02233338 cockroach\",\n",
    "    \"n02236044 mantis\",\n",
    "    \"n02259212 leafhopper\",\n",
    "    \"n02268443 dragonfly\",\n",
    "    \"n02279972 monarch butterfly\",\n",
    "    \"n02280649 small white\",\n",
    "    \"n02281787 gossamer-winged butterfly\",\n",
    "    \"n02317335 starfish\",\n",
    "    \"n02325366 cottontail rabbit\",\n",
    "    \"n02346627 porcupine\",\n",
    "    \"n02356798 fox squirrel\",\n",
    "    \"n02361337 marmot\",\n",
    "    \"n02410509 bison\",\n",
    "    \"n02445715 skunk\",\n",
    "    \"n02454379 armadillo\",\n",
    "    \"n02486410 baboon\",\n",
    "    \"n02492035 white-headed capuchin\",\n",
    "    \"n02504458 African bush elephant\",\n",
    "    \"n02655020 pufferfish\",\n",
    "    \"n02669723 academic gown\",\n",
    "    \"n02672831 accordion\",\n",
    "    \"n02676566 acoustic guitar\",\n",
    "    \"n02690373 airliner\",\n",
    "    \"n02701002 ambulance\",\n",
    "    \"n02730930 apron\",\n",
    "    \"n02777292 balance beam\",\n",
    "    \"n02782093 balloon\",\n",
    "    \"n02787622 banjo\",\n",
    "    \"n02793495 barn\",\n",
    "    \"n02797295 wheelbarrow\",\n",
    "    \"n02802426 basketball\",\n",
    "    \"n02814860 lighthouse\",\n",
    "    \"n02815834 beaker\",\n",
    "    \"n02837789 bikini\",\n",
    "    \"n02879718 bow\",\n",
    "    \"n02883205 bow tie\",\n",
    "    \"n02895154 breastplate\",\n",
    "    \"n02906734 broom\",\n",
    "    \"n02948072 candle\",\n",
    "    \"n02951358 canoe\",\n",
    "    \"n02980441 castle\",\n",
    "    \"n02992211 cello\",\n",
    "    \"n02999410 chain\",\n",
    "    \"n03014705 chest\",\n",
    "    \"n03026506 Christmas stocking\",\n",
    "    \"n03124043 cowboy boot\",\n",
    "    \"n03125729 cradle\",\n",
    "    \"n03187595 rotary dial telephone\",\n",
    "    \"n03196217 digital clock\",\n",
    "    \"n03223299 doormat\",\n",
    "    \"n03250847 drumstick\",\n",
    "    \"n03255030 dumbbell\",\n",
    "    \"n03291819 envelope\",\n",
    "    \"n03325584 feather boa\",\n",
    "    \"n03355925 flagpole\",\n",
    "    \"n03384352 forklift\",\n",
    "    \"n03388043 fountain\",\n",
    "    \"n03417042 garbage truck\",\n",
    "    \"n03443371 goblet\",\n",
    "    \"n03444034 go-kart\",\n",
    "    \"n03445924 golf cart\",\n",
    "    \"n03452741 grand piano\",\n",
    "    \"n03483316 hair dryer\",\n",
    "    \"n03584829 clothes iron\",\n",
    "    \"n03590841 jack-o'-lantern\",\n",
    "    \"n03594945 jeep\",\n",
    "    \"n03617480 kimono\",\n",
    "    \"n03666591 lighter\",\n",
    "    \"n03670208 limousine\",\n",
    "    \"n03717622 manhole cover\",\n",
    "    \"n03720891 maraca\",\n",
    "    \"n03721384 marimba\",\n",
    "    \"n03724870 mask\",\n",
    "    \"n03775071 mitten\",\n",
    "    \"n03788195 mosque\",\n",
    "    \"n03804744 nail\",\n",
    "    \"n03837869 obelisk\",\n",
    "    \"n03840681 ocarina\",\n",
    "    \"n03854065 organ\",\n",
    "    \"n03888257 parachute\",\n",
    "    \"n03891332 parking meter\",\n",
    "    \"n03935335 piggy bank\",\n",
    "    \"n03982430 billiard table\",\n",
    "    \"n04019541 hockey puck\",\n",
    "    \"n04033901 quill\",\n",
    "    \"n04039381 racket\",\n",
    "    \"n04067472 reel\",\n",
    "    \"n04086273 revolver\",\n",
    "    \"n04099969 rocking chair\",\n",
    "    \"n04118538 rugby ball\",\n",
    "    \"n04131690 salt shaker\",\n",
    "    \"n04133789 sandal\",\n",
    "    \"n04141076 saxophone\",\n",
    "    \"n04146614 school bus\",\n",
    "    \"n04147183 schooner\",\n",
    "    \"n04179913 sewing machine\",\n",
    "    \"n04208210 shovel\",\n",
    "    \"n04235860 sleeping bag\",\n",
    "    \"n04252077 snowmobile\",\n",
    "    \"n04252225 snowplow\",\n",
    "    \"n04254120 soap dispenser\",\n",
    "    \"n04270147 spatula\",\n",
    "    \"n04275548 spider web\",\n",
    "    \"n04310018 steam locomotive\",\n",
    "    \"n04317175 stethoscope\",\n",
    "    \"n04344873 couch\",\n",
    "    \"n04347754 submarine\",\n",
    "    \"n04355338 sundial\",\n",
    "    \"n04366367 suspension bridge\",\n",
    "    \"n04376876 syringe\",\n",
    "    \"n04389033 tank\",\n",
    "    \"n04399382 teddy bear\",\n",
    "    \"n04442312 toaster\",\n",
    "    \"n04456115 torch\",\n",
    "    \"n04482393 tricycle\",\n",
    "    \"n04507155 umbrella\",\n",
    "    \"n04509417 unicycle\",\n",
    "    \"n04532670 viaduct\",\n",
    "    \"n04540053 volleyball\",\n",
    "    \"n04554684 washing machine\",\n",
    "    \"n04562935 water tower\",\n",
    "    \"n04591713 wine bottle\",\n",
    "    \"n04606251 shipwreck\",\n",
    "    \"n07583066 guacamole\",\n",
    "    \"n07695742 pretzel\",\n",
    "    \"n07697313 cheeseburger\",\n",
    "    \"n07697537 hot dog\",\n",
    "    \"n07714990 broccoli\",\n",
    "    \"n07718472 cucumber\",\n",
    "    \"n07720875 bell pepper\",\n",
    "    \"n07734744 mushroom\",\n",
    "    \"n07749582 lemon\",\n",
    "    \"n07753592 banana\",\n",
    "    \"n07760859 custard apple\",\n",
    "    \"n07768694 pomegranate\",\n",
    "    \"n07831146 carbonara\",\n",
    "    \"n09229709 bubble\",\n",
    "    \"n09246464 cliff\",\n",
    "    \"n09472597 volcano\",\n",
    "    \"n09835506 baseball player\",\n",
    "    \"n11879895 rapeseed\",\n",
    "    \"n12057211 yellow lady's slipper\",\n",
    "    \"n12144580 corn\",\n",
    "    \"n12267677 acorn\"\n",
    "]\n",
    "\n",
    "class_mapping = {}\n",
    "for line in class_mapping_data:\n",
    "    parts = line.strip().split(' ')\n",
    "    class_mapping[parts[0]] = ' '.join(parts[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 2 MEMO baseline**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MEMO overview: Test-Time Augmentation for Robustness\n",
    "\n",
    "MEMO is a Test-Time Augmentation (TTA) technique designed to enhance a model's performance on test data. The MEMO process can be summarized in the following steps:\n",
    "\n",
    "1. **Loading the Pre-trained Model**: We use a pre-trained model, in this case a ResNet50.  \n",
    "\n",
    "2. **Generating Augmentations**: For each test image, we generate various augmentations (variations of the image). The augmentations can include transformations such as rotations, crops, color changes, etc.  \n",
    "\n",
    "3. **Computing Predictions**: We pass each augmented image through the model to obtain predictions, producing a set of predictions for each version of the image.  \n",
    "\n",
    "4. **Minimizing Entropy**: We use an entropy loss to adapt the model weights during testing. The goal is to minimize the entropy of the predictions, making the model more confident in its predictions.  \n",
    "\n",
    "5. **Updating Model Weights**: We update the model weights based on the calculated entropy loss.  \n",
    "\n",
    "6. **Final Prediction**: After updating the weights, we pass the original (non-augmented) image through the model to obtain the final prediction.\n",
    "\n",
    "\n",
    "Below, we present the implementation of MEMO baseline with augmix augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize the augmentator and the number of augmentations\n",
    "augmentator = ImageAugmentor()\n",
    "n_augmentations = 64\n",
    "\n",
    "# Determine the device to use (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the pre-trained ResNet50 model with default weights\n",
    "model = resnet50(weights=ResNet50_Weights.DEFAULT).to(device)\n",
    "model.train()  # Set the model to training mode\n",
    "\n",
    "# Initialize the optimizer (AdamW in this case)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "# Save the initial state of the model\n",
    "model_weights = deepcopy(model.state_dict())\n",
    "\n",
    "# Save the initial BatchNorm running mean and variance\n",
    "train_mean = []\n",
    "train_var = []\n",
    "for module in model.modules():\n",
    "    if isinstance(module, torch.nn.BatchNorm2d):\n",
    "        train_mean.append(deepcopy(module.running_mean))\n",
    "        train_var.append(deepcopy(module.running_var))\n",
    "\n",
    "# Variables to track the total number of images and correctly classified images\n",
    "total_images = 0\n",
    "correcly_classified_images = 0\n",
    "\n",
    "# Iterate over all folders in the 'imagenet-a' directory\n",
    "for folder in tqdm(os.listdir(dataset_path), desc=\"Processing classes\"):\n",
    "    image_folder = os.path.join(dataset_path, folder)\n",
    "    \n",
    "    # Skip if not a directory\n",
    "    if os.path.isdir(image_folder) == False:\n",
    "        continue\n",
    "    \n",
    "    # Get the true label of the current class from the folder name\n",
    "    true_label = get_class_label(image_folder[-9:])\n",
    "    \n",
    "    # List all image files in the folder\n",
    "    image_files = [os.path.join(image_folder, img) for img in os.listdir(image_folder) if img.endswith('.jpg') or img.endswith('.png')]\n",
    "\n",
    "    total_images += len(image_files)\n",
    "\n",
    "    equal_images_current_folder = 0\n",
    "\n",
    "    N = 16  # Used for BatchNorm calculations\n",
    "\n",
    "    # Iterate over all images in the folder\n",
    "    for i in image_files:\n",
    "        model.train()  # Set the model to training mode\n",
    "\n",
    "        # Apply augmentations to the image\n",
    "        images_aug = augmentator.apply_augmentations(i, True, n_augmentations)\n",
    "        likelyhood_list = []\n",
    "\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        \n",
    "        # Predict for each augmented image\n",
    "        for img in images_aug:\n",
    "            proc = process_image(img).to(device)  # Process the image\n",
    "            output = model(proc)\n",
    "            _, preds = torch.max(output, 1)  # Get the predicted class\n",
    "            likelyhood_list.append(output)\n",
    "       \n",
    "        # Adjust BatchNorm statistics\n",
    "        index = 0\n",
    "        for module in model.modules():\n",
    "            if isinstance(module, torch.nn.BatchNorm2d):\n",
    "                # Retrieve current statistics\n",
    "                mu_train = train_mean[index]\n",
    "                var_train = train_var[index]\n",
    "                \n",
    "                # Calculate test statistics\n",
    "                mu_test = module.running_mean\n",
    "                var_test = module.running_var\n",
    "                \n",
    "                # Mix the statistics\n",
    "                module.running_mean = (N / (N + 1)) * mu_train + (1 / (N + 1)) * mu_test\n",
    "                module.running_var = (N / (N + 1)) * var_train + (1 / (N + 1)) * var_test\n",
    "\n",
    "                index += 1\n",
    "\n",
    "        # Convert the list of probabilities to a tensor and calculate the mean entropy\n",
    "        probabilities_tensor = torch.stack(likelyhood_list)\n",
    "        mean_entropy, avg_logits = marginal_entropy(probabilities_tensor)\n",
    "\n",
    "        # Backpropagation and update the model parameters\n",
    "        mean_entropy.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "        # Predict the original image\n",
    "        with torch.no_grad():\n",
    "            output = model(load_and_preprocess_image(i).to(device))\n",
    "            _, preds = torch.max(output, 1)\n",
    "\n",
    "            # Check if the prediction is correct\n",
    "            if (class_idx[preds.item()] == true_label):\n",
    "                equal_images_current_folder += 1\n",
    "                correcly_classified_images += 1\n",
    "\n",
    "        # Restore the initial model state\n",
    "        model.load_state_dict(model_weights)\n",
    "\n",
    "    # Print the number of correctly classified images for the current class\n",
    "    print(f\"Number of images classified correctly for the class ----> {true_label}: {equal_images_current_folder}\")\n",
    "\n",
    "# Print the overall classification accuracy\n",
    "print(\"__________________________________________________________\")\n",
    "print(f\"Total number of images: {total_images}\")\n",
    "print(f\"Number of correctly classified images: {correcly_classified_images}\")\n",
    "print(f\"Accuracy: {correcly_classified_images / total_images}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 3: MEMO plus BLIP**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MEMO Plus BLIP Overview: Integrating Test-Time Augmentation with Visual Question Answering\n",
    "MEMO plus BLIP is an advanced Test-Time Augmentation (TTA) technique that incorporates Visual Question Answering (VQA) to enhance model performance on test data. The process involves the following steps:\n",
    "\n",
    "1. **Loading the Pre-trained Models**: We utilize a pre-trained ResNet50 model for image classification and a pre-trained BLIP model for VQA.\n",
    "\n",
    "2. **Generating Augmentations**: For each test image, we create multiple augmented versions by applying various transformations such as rotations, crops, and color adjustments.\n",
    "\n",
    "3. **Computing Predictions**: Each augmented image is processed through the ResNet50 model to obtain predictions, resulting in a set of predictions for each augmented version of the image.\n",
    "\n",
    "4. **Incorporating VQA with BLIP**: For each prediction, we leverage the BLIP model to ask a question about the presence of the predicted class in the original (non-augmented) image. The detailed steps are:\n",
    "    - **Generate Questions**: For each predicted class from the augmented images, formulate a question like \"Is there a [predicted class] in the picture?\"\n",
    "    - **Use BLIP for Answers**: The BLIP model processes both the question and the original image to generate an answer.\n",
    "    - **Adjust Prediction Confidence**:\n",
    "        - **Positive Response**: If BLIP confirms the presence of the class (e.g., with answers like \"yes\", \"there is\", \"correct\", \"true\"), the confidence score of the corresponding prediction is increased by a specific value.\n",
    "        - **Negative Response**: If BLIP indicates the absence of the class (e.g., with answers like \"no\", \"not\", \"false\"), the confidence score of the prediction remains unchanged.\n",
    "\n",
    "5. **Minimizing Entropy**: An entropy-based loss function is used to adapt the model weights during the testing phase. The objective is to minimize the entropy of the predictions, thereby increasing the model's confidence.\n",
    "\n",
    "6. **Updating Model Weights**: Based on the entropy loss calculated, the model weights are updated to refine the prediction accuracy.\n",
    "\n",
    "7. **Final Prediction**: After updating the model weights, the original (non-augmented) image is passed through the ResNet50 model again to obtain the final, adjusted prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
    "\n",
    "# Load the processor and model for Visual Question Answering (VQA)\n",
    "processor_blip = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "model_blip = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "\n",
    "# Initialize an image augmentation class (assumed to be implemented elsewhere)\n",
    "augmentator = ImageAugmentor()\n",
    "n_augmentations = 20  # Number of augmentations to apply\n",
    "\n",
    "# Set the device to GPU if available, otherwise use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the ResNet50 model with pre-trained weights and move it to the selected device\n",
    "model = resnet50(weights=ResNet50_Weights.DEFAULT).to(device)\n",
    "model.train()  # Set the model to training mode\n",
    "\n",
    "# Initialize the AdamW optimizer for model training with a learning rate of 0.001\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "# Save the initial state of the model to restore later\n",
    "model_weights = deepcopy(model.state_dict())\n",
    "\n",
    "# Initialize lists to store batch normalization statistics\n",
    "train_mean = []\n",
    "train_var = []\n",
    "\n",
    "# Loop through the model's modules to extract statistics from BatchNorm2d layers\n",
    "for module in model.modules():\n",
    "    if isinstance(module, torch.nn.BatchNorm2d):\n",
    "        train_mean.append(deepcopy(module.running_mean))\n",
    "        train_var.append(deepcopy(module.running_var))\n",
    "\n",
    "# Initialize counters for total and correctly classified images\n",
    "total_images = 0\n",
    "correcly_classified_images = 0\n",
    "\n",
    "# Set experiment type\n",
    "memo = True\n",
    "dataseth = \"imagenet-a\"  # Can also be \"imagenetv2\" (not imported in this file)\n",
    "\n",
    "# Iterate through each folder in the dataset directory\n",
    "for folder in tqdm(os.listdir(f\"{dataset_path}\"), desc=\"Processing classes\"):\n",
    "\n",
    "    # Construct the full path to the current image folder\n",
    "    image_folder = os.path.join(dataset_path, folder)\n",
    "    \n",
    "    # Skip non-directory files\n",
    "    if not os.path.isdir(image_folder):\n",
    "        continue\n",
    "\n",
    "    # Determine the true class label based on the experiment type\n",
    "    true_label = get_class_label(folder) if dataseth == \"imagenet-a\" else class_idx[int(folder)]\n",
    "    \n",
    "    # List of image files in the current folder\n",
    "    image_files = [os.path.join(image_folder, img) for img in os.listdir(image_folder) if img.endswith('.jpg') or img.endswith('.png') or img.endswith(\".jpeg\")]\n",
    "    total_images += len(image_files)  # Update total image count\n",
    "\n",
    "    equal_images_current_folder = 0  # Counter for correctly classified images in the current folder\n",
    "    N = 16  # A constant for updating batch normalization statistics\n",
    "\n",
    "    for i in image_files:\n",
    "        \n",
    "        model.train()  # Ensure the model is in training mode\n",
    "        if memo:\n",
    "            # Apply augmentations to the image\n",
    "            images_aug = augmentator.apply_all_augmentations(i)\n",
    "            likelihood_list = []  # List to store the output probabilities\n",
    "\n",
    "            optimizer.zero_grad()  # Clear the previous gradients\n",
    "\n",
    "            prediction_tmp = []  # Temporary list to store predictions for this image\n",
    "\n",
    "            image_test = Image.open(i)  # Open the image file\n",
    "\n",
    "            dictionary_blip = {}  # Dictionary to store answers from the BLIP model\n",
    "\n",
    "            for img in images_aug:\n",
    "                proc = process_image(img).to(device)  # Process the augmented image\n",
    "                output = model(proc)  # Get the model's output\n",
    "                \n",
    "                _, preds = torch.max(output, 1)  # Get the predicted class\n",
    "                prediction_tmp.append(class_idx[preds.item()])  # Store the predicted class\n",
    "\n",
    "                if class_idx[preds.item()] in dictionary_blip:\n",
    "                    if dictionary_blip[class_idx[preds.item()]] in [\"yes\", \"there is\", \"correct\", \"true\"]:\n",
    "                        # Increase the probability of the predicted class if the answer is affirmative\n",
    "                        output[0][preds.item()] += output[0][preds.item()] * 2\n",
    "                else:\n",
    "                    # Formulate a question for the BLIP model\n",
    "                    question = f\"Is there a {class_idx[preds.item()]} in the picture?\"\n",
    "\n",
    "                    # Prepare the input for the BLIP model\n",
    "                    inputs = processor_blip(image_test, question, return_tensors=\"pt\")\n",
    "                    # Generate an answer from the BLIP model\n",
    "                    out = model_blip.generate(**inputs, max_length=2, num_beams=1)\n",
    "                    # Decode the answer\n",
    "                    risposta_blip = processor_blip.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "                    dictionary_blip[class_idx[preds.item()]] = risposta_blip.lower()  # Store the answer\n",
    "\n",
    "                    if risposta_blip.lower() in [\"yes\", \"there is\", \"correct\", \"true\"]:\n",
    "                        # Increase the probability of the predicted class if the answer is affirmative\n",
    "                        output[0][preds.item()] += output[0][preds.item()] * 2\n",
    "\n",
    "                likelihood_list.append(output)  # Add the output to the list\n",
    "\n",
    "            # Update batch normalization statistics\n",
    "            index = 0\n",
    "            for module in model.modules():\n",
    "                if isinstance(module, torch.nn.BatchNorm2d):\n",
    "                    mu_train = train_mean[index]  # Training mean\n",
    "                    var_train = train_var[index]  # Training variance\n",
    "                    mu_test = module.running_mean  # Test mean\n",
    "                    var_test = module.running_var  # Test variance\n",
    "                    \n",
    "                    # Update running statistics\n",
    "                    module.running_mean = (N / (N + 1)) * mu_train + (1 / (N + 1)) * mu_test\n",
    "                    module.running_var = (N / (N + 1)) * var_train + (1 / (N + 1)) * var_test\n",
    "\n",
    "                    index += 1\n",
    "\n",
    "            # Convert the list of probabilities to a tensor and compute the mean entropy\n",
    "            probabilities_tensor = torch.stack(likelihood_list)\n",
    "            mean_entropy, avg_logits = marginal_entropy(probabilities_tensor)\n",
    "\n",
    "            # Perform backpropagation and update model parameters\n",
    "            mean_entropy.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Load and preprocess the image for evaluation\n",
    "            output = model(load_and_preprocess_image(i).to(device))\n",
    "            _, preds = torch.max(output, 1)  # Get the predicted class\n",
    "            if class_idx[preds.item()] == true_label:\n",
    "                equal_images_current_folder += 1  # Increment counter for correctly classified images\n",
    "                correcly_classified_images += 1\n",
    "\n",
    "        model.load_state_dict(model_weights)  # Restore the model to its original state\n",
    "    \n",
    "    # Print the number of correctly classified images for the current class\n",
    "    print(f\"The number of images classified correctly for class ----> {true_label}: {equal_images_current_folder}\")\n",
    "\n",
    "# Print the overall classification accuracy and statistics\n",
    "print(\"__________________________________________________________\")\n",
    "print(f\"Total number of images: {total_images}\")\n",
    "print(f\"Number of correctly classified images: {correcly_classified_images}\")\n",
    "print(f\"Accuracy: {correcly_classified_images / total_images}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Best result**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Macinando classi:   0% 0/202 [00:00<?, ?it/s] The number of images classified correctly for class ----> mask: 0  \n",
    "Macinando classi:   0% 1/202 [04:24<14:45:30, 264.33s/it] The number of images classified correctly for class ----> jack-o'-lantern: 4  \n",
    "Macinando classi:   1% 2/202 [07:01<11:10:36, 201.18s/it] The number of images classified correctly for class ----> rapeseed: 3  \n",
    "Macinando classi:   1% 3/202 [08:58<9:00:45, 163.04s/it] The number of images classified correctly for class ----> garbage truck: 1  \n",
    "Macinando classi:   2% 4/202 [09:56<6:40:57, 121.50s/it] The number of images classified correctly for class ----> barn: 0  \n",
    "Macinando classi:   2% 5/202 [12:39<7:27:28, 136.29s/it] The number of images classified correctly for class ----> bikini: 1  \n",
    "Macinando classi:   3% 6/202 [14:57<7:27:06, 136.87s/it] The number of images classified correctly for class ----> oystercatcher: 5  \n",
    "Macinando classi:   3% 7/202 [16:04<6:10:26, 113.98s/it] The number of images classified correctly for class ----> scorpion: 49  \n",
    "Macinando classi:   4% 8/202 [26:54<15:21:00, 284.85s/it] The number of images classified correctly for class ----> jellyfish: 11  \n",
    "Macinando classi:   4% 9/202 [36:45<20:23:32, 380.38s/it] The number of images classified correctly for class ----> lynx: 3  \n",
    "Macinando classi:   5% 10/202 [47:58<25:06:57, 470.92s/it] The number of images classified correctly for class ----> starfish: 7  \n",
    "Macinando classi:   5% 11/202 [51:26<20:42:56, 390.45s/it] The number of images classified correctly for class ----> cello: 1  \n",
    "Macinando classi:   6% 12/202 [52:10<15:02:02, 284.86s/it] The number of images classified correctly for class ----> tarantula: 4  \n",
    "Macinando classi:   6% 13/202 [56:54<14:56:54, 284.73s/it] The number of images classified correctly for class ----> unicycle: 2  \n",
    "Macinando classi:   7% 14/202 [58:54<12:16:04, 234.92s/it] The number of images classified correctly for class ----> syringe: 0  \n",
    "Macinando classi:   7% 15/202 [1:00:04<9:37:05, 185.16s/it] The number of images classified correctly for class ----> ambulance: 1  \n",
    "Macinando classi:   8% 16/202 [1:00:51<7:25:09, 143.60s/it] The number of images classified correctly for class ----> hot dog: 0  \n",
    "Macinando classi:   8% 17/202 [1:02:06<6:18:55, 122.89s/it] The number of images classified correctly for class ----> sleeping bag: 0  \n",
    "Macinando classi:   9% 18/202 [1:03:00<5:14:00, 102.39s/it] The number of images classified correctly for class ----> sea lion: 9  \n",
    "Macinando classi:   9% 19/202 [1:07:08<7:25:08, 145.95s/it] The number of images classified correctly for class ----> snowplow: 0  \n",
    "Macinando classi:  10% 20/202 [1:08:20<6:15:09, 123.68s/it] The number of images classified correctly for class ----> baseball player: 0  \n",
    "Macinando classi:  11% 22/202 [1:09:36<4:12:41, 84.23s/it] The number of images classified correctly for class ----> monarch butterfly: 4  \n",
    "Macinando classi:  11% 23/202 [1:19:25<10:24:39, 209.38s/it] The number of images classified correctly for class ----> harvestman: 13  \n",
    "Macinando classi:  12% 24/202 [1:26:41<13:17:03, 268.67s/it] The number of images classified correctly for class ----> organ: 0  \n",
    "Macinando classi:  12% 25/202 [1:27:56<10:37:01, 215.94s/it] The number of images classified correctly for class ----> doormat: 0  \n",
    "Macinando classi:  13% 26/202 [1:29:38<8:59:57, 184.07s/it] The number of images classified correctly for class ----> golf cart: 6  \n",
    "Macinando classi:  13% 27/202 [1:31:35<8:00:53, 164.88s/it] The number of images classified correctly for class ----> koala: 2  \n",
    "Macinando classi:  14% 28/202 [1:34:32<8:08:28, 168.44s/it] The number of images classified correctly for class ----> billiard table: 0  \n",
    "Macinando classi:  14% 29/202 [1:37:02<7:50:14, 163.09s/it] The number of images classified correctly for class ----> academic gown: 1    \n",
    "Macinando classi:  15% 30/202 [1:38:14<6:30:36, 136.26s/it] The number of images classified correctly for class ----> grasshopper: 11    \n",
    "Macinando classi:  15% 31/202 [1:49:35<14:08:23, 297.68s/it] The number of images classified correctly for class ----> pug: 5    \n",
    "Macinando classi:  16% 32/202 [1:52:17<12:08:47, 257.22s/it] The number of images classified correctly for class ----> suspension bridge: 1    \n",
    "Macinando classi:  16% 33/202 [1:55:09<10:53:35, 232.05s/it] The number of images classified correctly for class ----> red fox: 7    \n",
    "Macinando classi:  17% 34/202 [2:03:01<14:10:11, 303.64s/it] The number of images classified correctly for class ----> soap dispenser: 0  \n",
    "Macinando classi:  17% 35/202 [2:03:42<10:25:58, 224.90s/it] The number of images classified correctly for class ----> parking meter: 1  \n",
    "Macinando classi:  18% 36/202 [2:05:07<8:26:31, 183.08s/it] The number of images classified correctly for class ----> stingray: 7  \n",
    "Macinando classi:  18% 37/202 [2:14:07<13:17:24, 289.97s/it] The number of images classified correctly for class ----> Golden Retriever: 1  \n",
    "Macinando classi:  19% 38/202 [2:15:57<10:45:16, 236.08s/it] The number of images classified correctly for class ----> candle: 9  \n",
    "Macinando classi:  19% 39/202 [2:22:13<12:35:41, 278.17s/it] The number of images classified correctly for class ----> torch: 6  \n",
    "Macinando classi:  20% 40/202 [2:24:55<10:56:29, 243.15s/it] The number of images classified correctly for class ----> mantis: 30  \n",
    "Macinando classi:  20% 41/202 [2:36:03<16:34:22, 370.57s/it] The number of images classified correctly for class ----> small white: 8  \n",
    "Macinando classi:  21% 42/202 [2:43:00<17:05:48, 384.68s/it] The number of images classified correctly for class ----> cradle: 0  \n",
    "Macinando classi:  21% 43/202 [2:43:49<12:32:14, 283.87s/it] The number of images classified correctly for class ----> balloon: 0  \n",
    "Macinando classi:  22% 44/202 [2:47:13<11:24:26, 259.91s/it] The number of images classified correctly for class ----> acoustic guitar: 3  \n",
    "Macinando classi:  22% 45/202 [2:49:30<9:44:02, 223.20s/it] The number of images classified correctly for class ----> cowboy boot: 1  \n",
    "Macinando classi:  23% 46/202 [2:52:23<9:00:29, 207.88s/it] The number of images classified correctly for class ----> dumbbell: 4  \n",
    "Macinando classi:  23% 47/202 [2:54:26<7:51:27, 182.50s/it] The number of images classified correctly for class ----> dragonfly: 22  \n",
    "Macinando classi:  24% 48/202 [3:03:35<12:31:04, 292.62s/it] The number of images classified correctly for class ----> rocking chair: 2  \n",
    "Macinando classi:  24% 49/202 [3:05:43<10:19:39, 243.00s/it] The number of images classified correctly for class ----> Chihuahua: 4  \n",
    "Macinando classi:  25% 50/202 [3:07:22<8:26:27, 199.92s/it] The number of images classified correctly for class ----> couch: 1  \n",
    "Macinando classi:  25% 51/202 [3:11:46<9:11:44, 219.23s/it] The number of images classified correctly for class ----> centipede: 16  \n",
    "Macinando classi:  26% 52/202 [3:18:40<11:33:44, 277.50s/it] The number of images classified correctly for class ----> American robin: 24  \n",
    "Macinando classi:  26% 53/202 [3:29:58<16:27:18, 397.57s/it] The number of images classified correctly for class ----> armadillo: 4  \n",
    "Macinando classi:  27% 54/202 [3:32:47<13:31:33, 329.01s/it] The number of images classified correctly for class ----> parachute: 0  \n",
    "Macinando classi:  27% 55/202 [3:34:15<10:29:28, 256.93s/it] The number of images classified correctly for class ----> banana: 4  \n",
    "Macinando classi:  28% 56/202 [3:36:31<8:56:25, 220.45s/it] The number of images classified correctly for class ----> American black bear: 2  \n",
    "Macinando classi:  28% 57/202 [3:39:53<8:39:22, 214.91s/it] The number of images classified correctly for class ----> gossamer-winged butterfly: 11  \n",
    "Macinando classi:  29% 58/202 [3:49:13<12:44:34, 318.58s/it] The number of images classified correctly for class ----> teddy bear: 18  \n",
    "Macinando classi:  29% 59/202 [3:55:42<13:29:44, 339.75s/it] The number of images classified correctly for class ----> quill: 4  \n",
    "Macinando classi:  30% 60/202 [3:56:48<10:09:21, 257.48s/it] The number of images classified correctly for class ----> lighthouse: 2  \n",
    "Macinando classi:  30% 61/202 [3:59:46<9:09:14, 233.72s/it] The number of images classified correctly for class ----> ocarina: 2  \n",
    "Macinando classi:  31% 62/202 [4:00:35<6:56:05, 178.33s/it] The number of images classified correctly for class ----> sundial: 2  \n",
    "Macinando classi:  31% 63/202 [4:02:39<6:15:13, 161.96s/it] The number of images classified correctly for class ----> jeep: 5  \n",
    "Macinando classi:  32% 64/202 [4:06:04<6:42:07, 174.84s/it] The number of images classified correctly for class ----> chest: 1  \n",
    "Macinando classi:  32% 65/202 [4:07:35<5:41:42, 149.65s/it] The number of images classified correctly for class ----> ant: 17  \n",
    "Macinando classi:  33% 66/202 [4:15:07<9:05:12, 240.54s/it] The number of images classified correctly for class ----> snail: 17  \n",
    "Macinando classi:  33% 67/202 [4:23:49<12:11:17, 325.02s/it] The number of images classified correctly for class ----> great egret: 16  \n",
    "Macinando classi:  34% 68/202 [4:34:14<15:26:34, 414.89s/it] The number of images classified correctly for class ----> flagpole: 0  \n",
    "Macinando classi:  34% 69/202 [4:37:02<12:35:48, 340.97s/it] The number of images classified correctly for class ----> leafhopper: 14  \n",
    "Macinando classi:  35% 70/202 [4:41:32<11:43:06, 319.60s/it] The number of images classified correctly for class ----> skunk: 17  \n",
    "Macinando classi:  35% 71/202 [4:48:59<13:00:57, 357.69s/it] The number of images classified correctly for class ----> marmot: 0  \n",
    "Macinando classi:  36% 72/202 [4:51:40<10:47:21, 298.78s/it] The number of images classified correctly for class ----> fly: 18  \n",
    "Macinando classi:  36% 73/202 [5:02:39<14:34:38, 406.81s/it] The number of images classified correctly for class ----> balance beam: 6  \n",
    "Macinando classi:  37% 74/202 [5:06:08<12:21:04, 347.38s/it] The number of images classified correctly for class ----> broom: 0  \n",
    "Macinando classi:  37% 75/202 [5:09:48<10:54:39, 309.29s/it] The number of images classified correctly for class ----> goose: 12  \n",
    "Macinando classi:  38% 76/202 [5:21:04<14:40:30, 419.29s/it] The number of images classified correctly for class ----> banjo: 0  \n",
    "Macinando classi:  38% 77/202 [5:22:09<10:52:10, 313.04s/it] The number of images classified correctly for class ----> cliff: 0  \n",
    "Macinando classi:  39% 78/202 [5:23:30<8:22:43, 243.26s/it] The number of images classified correctly for class ----> corn: 2  \n",
    "Macinando classi:  39% 79/202 [5:25:37<7:07:28, 208.53s/it] The number of images classified correctly for class ----> flamingo: 8  \n",
    "Macinando classi:  40% 80/202 [5:33:25<9:42:09, 286.31s/it] The number of images classified correctly for class ----> pomegranate: 1  \n",
    "Macinando classi:  40% 81/202 [5:38:38<9:53:51, 294.47s/it] The number of images classified correctly for class ----> bow tie: 2  \n",
    "Macinando classi:  41% 82/202 [5:41:16<8:26:37, 253.31s/it] The number of images classified correctly for class ----> viaduct: 4  \n",
    "Macinando classi:  41% 83/202 [5:43:24<7:07:53, 215.74s/it] The number of images classified correctly for class ----> bow: 6  \n",
    "Macinando classi:  42% 84/202 [5:46:01<6:29:54, 198.26s/it] The number of images classified correctly for class ----> beaker: 0  \n",
    "Macinando classi:  42% 85/202 [5:46:43<4:55:20, 151.46s/it] The number of images classified correctly for class ----> ladybug: 38  \n",
    "Macinando classi:  43% 86/202 [5:57:35<9:42:44, 301.42s/it] The number of images classified correctly for class ----> volleyball: 3  \n",
    "Macinando classi:  43% 87/202 [6:00:31<8:25:56, 263.97s/it] The number of images classified correctly for class ----> basketball: 10  \n",
    "Macinando classi:  44% 88/202 [6:05:24<8:37:41, 272.47s/it] The number of images classified correctly for class ----> limousine: 0  \n",
    "Macinando classi:  44% 89/202 [6:06:15<6:28:02, 206.04s/it] The number of images classified correctly for class ----> salt shaker: 1  \n",
    "Macinando classi:  45% 90/202 [6:08:54<5:58:37, 192.12s/it] The number of images classified correctly for class ----> newt: 2  \n",
    "Macinando classi:  45% 91/202 [6:15:35<7:51:15, 254.74s/it] The number of images classified correctly for class ----> water tower: 1  \n",
    "Macinando classi:  46% 92/202 [6:20:00<7:52:16, 257.60s/it] The number of images classified correctly for class ----> American bullfrog: 7  \n",
    "Macinando classi:  46% 93/202 [6:31:26<11:41:28, 386.13s/it] The number of images classified correctly for class ----> mushroom: 5  \n",
    "Macinando classi:  47% 94/202 [6:42:12<13:55:48, 464.34s/it] The number of images classified correctly for class ----> American alligator: 8  \n",
    "Macinando classi:  47% 95/202 [6:50:29<14:05:18, 474.00s/it] The number of images classified correctly for class ----> shipwreck: 1  \n",
    "Macinando classi:  48% 96/202 [6:51:51<10:29:32, 356.35s/it] The number of images classified correctly for class ----> umbrella: 2  \n",
    "Macinando classi:  48% 97/202 [6:55:14<9:03:00, 310.29s/it] The number of images classified correctly for class ----> bubble: 0  \n",
    "Macinando classi:  49% 98/202 [6:59:42<8:36:19, 297.88s/it] The number of images classified correctly for class ----> Rottweiler: 3  \n",
    "Macinando classi:  49% 99/202 [7:03:33<7:56:26, 277.54s/it] The number of images classified correctly for class ----> pufferfish: 3  \n",
    "Macinando classi:  50% 100/202 [7:08:35<8:04:28, 284.98s/it] The number of images classified correctly for class ----> breastplate: 0  \n",
    "Macinando classi:  50% 101/202 [7:09:25<6:01:00, 214.46s/it] The number of images classified correctly for class ----> sewing machine: 4  \n",
    "Macinando classi:  50% 102/202 [7:11:36<5:15:47, 189.48s/it] The number of images classified correctly for class ----> maraca: 1  \n",
    "Macinando classi:  51% 103/202 [7:13:09<4:24:48, 160.49s/it] The number of images classified correctly for class ----> wine bottle: 3  \n",
    "Macinando classi:  51% 104/202 [7:17:24<5:08:29, 188.87s/it] The number of images classified correctly for class ----> spider web: 2  \n",
    "Macinando classi:  52% 105/202 [7:21:51<5:43:05, 212.22s/it] The number of images classified correctly for class ----> fox squirrel: 27  \n",
    "Macinando classi:  52% 106/202 [7:33:38<9:37:04, 360.67s/it] The number of images classified correctly for class ----> rugby ball: 2  \n",
    "Macinando classi:  53% 107/202 [7:35:14<7:25:21, 281.28s/it] The number of images classified correctly for class ----> sulphur-crested cockatoo: 10  \n",
    "Macinando classi:  53% 108/202 [7:43:42<9:07:29, 349.46s/it] The number of images classified correctly for class ----> flatworm: 0  \n",
    "Macinando classi:  54% 109/202 [7:55:28<11:47:17, 456.31s/it] The number of images classified correctly for class ----> wheelbarrow: 4  \n",
    "Macinando classi:  54% 110/202 [7:59:58<10:14:12, 400.57s/it] The number of images classified correctly for class ----> box turtle: 5  \n",
    "Macinando classi:  55% 111/202 [8:07:10<10:21:26, 409.75s/it] The number of images classified correctly for class ----> washing machine: 0  \n",
    "Macinando classi:  55% 112/202 [8:08:50<7:55:30, 317.01s/it] The number of images classified correctly for class ----> drumstick: 1  \n",
    "Macinando classi:  56% 113/202 [8:10:15<6:06:40, 247.19s/it] The number of images classified correctly for class ----> sandal: 1  \n",
    "Macinando classi:  56% 114/202 [8:14:43<6:11:49, 253.51s/it] The number of images classified correctly for class ----> weevil: 0  \n",
    "Macinando classi:  57% 115/202 [8:17:23<4:23:54, 223.45s/it] The number of images classified correctly for class ----> lion: 1  \n",
    "Macinando classi:  57% 116/202 [8:19:50<4:57:21, 207.46s/it] The number of images classified correctly for class ----> revolver: 0  \n",
    "Macinando classi:  58% 117/202 [8:21:03<4:06:29, 174.00s/it] The number of images classified correctly for class ----> hair dryer: 1  \n",
    "Macinando classi:  58% 118/202 [8:22:08<3:23:55, 145.66s/it] The number of images classified correctly for class ----> bison: 4  \n",
    "Macinando classi:  59% 119/202 [8:24:11<3:12:57, 139.49s/it] The number of images classified correctly for class ----> broccoli: 3  \n",
    "Macinando classi:  59% 120/202 [8:25:25<2:45:33, 121.14s/it] The number of images classified correctly for class ----> submarine: 1  \n",
    "Macinando classi:  60% 121/202 [8:27:28<2:44:16, 121.69s/it] The number of images classified correctly for class ----> jay: 26  \n",
    "Macinando classi:  60% 122/202 [8:34:19<4:33:58, 205.48s/it] The number of images classified correctly for class ----> Christmas stocking: 0  \n",
    "Macinando classi:  61% 123/202 [8:36:54<4:10:57, 190.61s/it] The number of images classified correctly for class ----> clothes iron: 0  \n",
    "Macinando classi:  61% 124/202 [8:37:17<3:03:40, 141.29s/it] The number of images classified correctly for class ----> mitten: 0  \n",
    "Macinando classi:  62% 125/202 [8:40:16<3:15:41, 152.49s/it] The number of images classified correctly for class ----> tricycle: 1  \n",
    "Macinando classi:  62% 126/202 [8:40:41<2:24:58, 114.45s/it] The number of images classified correctly for class ----> goldfinch: 4  \n",
    "Macinando classi:  63% 127/202 [8:51:51<5:50:18, 280.24s/it] The number of images classified correctly for class ----> chameleon: 18  \n",
    "Macinando classi:  63% 128/202 [9:02:01<7:46:57, 378.61s/it] The number of images classified correctly for class ----> cucumber: 1  \n",
    "Macinando classi:  64% 129/202 [9:05:17<6:34:18, 324.09s/it] The number of images classified correctly for class ----> vulture: 17  \n",
    "Macinando classi:  64% 130/202 [9:13:33<7:30:27, 375.38s/it] The number of images classified correctly for class ----> go-kart: 2  \n",
    "Macinando classi:  65% 131/202 [9:14:36<5:33:47, 282.08s/it] The number of images classified correctly for class ----> porcupine: 8  \n",
    "Macinando classi:  65% 132/202 [9:21:12<6:08:46, 316.09s/it] The number of images classified correctly for class ----> baboon: 1  \n",
    "Macinando classi:  66% 133/202 [9:21:58<4:30:16, 235.02s/it] The number of images classified correctly for class ----> acorn: 0  \n",
    "Macinando classi:  66% 134/202 [9:24:25<3:56:37, 208.78s/it] The number of images classified correctly for class ----> sea anemone: 2  \n",
    "Macinando classi:  67% 135/202 [9:35:45<6:30:44, 349.92s/it] The number of images classified correctly for class ----> toaster: 3  \n",
    "Macinando classi:  67% 136/202 [9:37:29<5:03:53, 276.26s/it] The number of images classified correctly for class ----> schooner: 0  \n",
    "Macinando classi:  68% 137/202 [9:39:12<4:02:54, 224.22s/it] The number of images classified correctly for class ----> cockroach: 24  \n",
    "Macinando classi:  68% 138/202 [9:46:26<5:06:25, 287.27s/it] The number of images classified correctly for class ----> spatula: 1  \n",
    "Macinando classi:  69% 139/202 [9:48:18<4:06:24, 234.67s/it] The number of images classified correctly for class ----> mongoose: 9  \n",
    "Macinando classi:  69% 140/202 [9:54:34<4:46:22, 277.14s/it] The number of images classified correctly for class ----> bell pepper: 0  \n",
    "Macinando classi:  70% 141/202 [9:59:01<4:38:40, 274.10s/it] The number of images classified correctly for class ----> junco: 27  \n",
    "Macinando classi:  70% 142/202 [10:10:21<6:35:37, 395.63s/it] The number of images classified correctly for class ----> canoe: 1  \n",
    "Macinando classi:  71% 143/202 [10:11:28<4:52:22, 297.33s/it] The number of images classified correctly for class ----> snowmobile: 0  \n",
    "Macinando classi:  71% 144/202 [10:12:55<3:46:17, 234.09s/it] The number of images classified correctly for class ----> nail: 0  \n",
    "Macinando classi:  72% 145/202 [10:13:44<2:49:29, 178.42s/it] The number of images classified correctly for class ----> pretzel: 0  \n",
    "Macinando classi:  72% 146/202 [10:14:49<2:14:55, 144.57s/it] The number of images classified correctly for class ----> guacamole: 0  \n",
    "Macinando classi:  73% 147/202 [10:16:54<2:07:11, 138.76s/it] The number of images classified correctly for class ----> lorikeet: 17  \n",
    "Macinando classi:  73% 148/202 [10:28:28<4:34:39, 305.19s/it] The number of images classified correctly for class ----> pelican: 9  \n",
    "Macinando classi:  74% 149/202 [10:34:46<4:48:58, 327.14s/it] The number of images classified correctly for class ----> saxophone: 0  \n",
    "Macinando classi:  74% 150/202 [10:36:23<3:43:37, 258.04s/it] The number of images classified correctly for class ----> carbonara: 0  \n",
    "Macinando classi:  75% 151/202 [10:37:51<2:56:02, 207.10s/it] The number of images classified correctly for class ----> German Shepherd Dog: 0  \n",
    "Macinando classi:  75% 152/202 [10:39:09<2:20:15, 168.31s/it] The number of images classified correctly for class ----> grand piano: 1  \n",
    "Macinando classi:  76% 154/202 [10:40:49<1:30:52, 113.59s/it] The number of images classified correctly for class ----> stick insect: 51  \n",
    "Macinando classi:  77% 155/202 [10:47:27<2:24:18, 184.22s/it] The number of images classified correctly for class ----> Persian cat: 17  \n",
    "Macinando classi:  77% 156/202 [10:54:57<3:14:32, 253.76s/it] The number of images classified correctly for class ----> agama: 13  \n",
    "Macinando classi:  78% 157/202 [11:03:00<3:57:05, 316.12s/it] The number of images classified correctly for class ----> reel: 0  \n",
    "Macinando classi:  78% 158/202 [11:04:26<3:04:29, 251.58s/it] The number of images classified correctly for class ----> piggy bank: 2  \n",
    "Macinando classi:  79% 159/202 [11:05:44<2:24:49, 202.08s/it] The number of images classified correctly for class ----> envelope: 1  \n",
    "Macinando classi:  79% 160/202 [11:09:18<2:23:56, 205.64s/it] The number of images classified correctly for class ----> bee: 3  \n",
    "Macinando classi:  80% 161/202 [11:20:05<3:48:41, 334.67s/it] The number of images classified correctly for class ----> rhinoceros beetle: 1  \n",
    "Macinando classi:  80% 162/202 [11:22:32<3:06:15, 279.39s/it] The number of images classified correctly for class ----> stethoscope: 1  \n",
    "Macinando classi:  81% 163/202 [11:24:11<2:26:55, 226.05s/it] The number of images classified correctly for class ----> digital clock: 2  \n",
    "Macinando classi:  81% 164/202 [11:24:59<1:49:34, 173.02s/it] The number of images classified correctly for class ----> duck: 15  \n",
    "Macinando classi:  82% 165/202 [11:32:41<2:39:51, 259.22s/it] The number of images classified correctly for class ----> kimono: 2  \n",
    "Macinando classi:  82% 166/202 [11:35:16<2:16:50, 228.06s/it] The number of images classified correctly for class ----> rotary dial telephone: 0  \n",
    "Macinando classi:  83% 167/202 [11:36:35<1:46:56, 183.33s/it] The number of images classified correctly for class ----> green iguana: 20  \n",
    "Macinando classi:  83% 168/202 [11:47:41<3:05:50, 327.94s/it] The number of images classified correctly for class ----> school bus: 4  \n",
    "Macinando classi:  84% 169/202 [11:50:52<2:37:50, 286.99s/it] The number of images classified correctly for class ----> hermit crab: 4  \n",
    "Macinando classi:  84% 170/202 [11:53:32<2:12:43, 248.85s/it] The number of images classified correctly for class ----> fountain: 3  \n",
    "Macinando classi:  85% 171/202 [11:56:08<1:54:16, 221.18s/it] The number of images classified correctly for class ----> white-headed capuchin: 0  \n",
    "Macinando classi:  85% 172/202 [11:57:12<1:26:59, 173.98s/it] The number of images classified correctly for class ----> shovel: 0  \n",
    "Macinando classi:  86% 173/202 [11:58:58<1:14:12, 153.54s/it] The number of images classified correctly for class ----> castle: 0  \n",
    "Macinando classi:  86% 174/202 [12:00:30<1:03:04, 135.16s/it] The number of images classified correctly for class ----> cottontail rabbit: 9  \n",
    "Macinando classi:  87% 175/202 [12:07:21<1:38:05, 217.96s/it] The number of images classified correctly for class ----> steam locomotive: 0  \n",
    "Macinando classi:  87% 176/202 [12:11:12<1:36:03, 221.69s/it] The number of images classified correctly for class ----> chain: 1  \n",
    "Macinando classi:  88% 177/202 [12:17:35<1:52:31, 270.05s/it] The number of images classified correctly for class ----> African bush elephant: 0  \n",
    "Macinando classi:  88% 178/202 [12:20:46<1:38:31, 246.31s/it] The number of images classified correctly for class ----> garter snake: 4  \n",
    "Macinando classi:  89% 179/202 [12:29:47<2:08:17, 334.68s/it] The number of images classified correctly for class ----> marimba: 4  \n",
    "Macinando classi:  89% 180/202 [12:31:27<1:37:00, 264.55s/it] The number of images classified correctly for class ----> cheeseburger: 1  \n",
    "Macinando classi:  90% 181/202 [12:32:54<1:13:53, 211.10s/it] The number of images classified correctly for class ----> yellow lady's slipper: 0  \n",
    "Macinando classi:  90% 182/202 [12:33:51<55:01, 165.06s/it] The number of images classified correctly for class ----> lemon: 0  \n",
    "Macinando classi:  91% 183/202 [12:36:21<50:47, 160.40s/it] The number of images classified correctly for class ----> lighter: 7  \n",
    "Macinando classi:  91% 184/202 [12:42:17<1:05:43, 219.07s/it] The number of images classified correctly for class ----> toucan: 5  \n",
    "Macinando classi:  92% 185/202 [12:47:57<1:12:21, 255.38s/it] The number of images classified correctly for class ----> airliner: 1  \n",
    "Macinando classi:  92% 186/202 [12:50:47<1:01:14, 229.68s/it] The number of images classified correctly for class ----> hockey puck: 0  \n",
    "Macinando classi:  93% 187/202 [12:52:10<46:24, 185.64s/it] The number of images classified correctly for class ----> bald eagle: 11  \n",
    "Macinando classi:  93% 188/202 [13:01:35<1:09:53, 299.52s/it] The number of images classified correctly for class ----> volcano: 2  \n",
    "Macinando classi:  94% 189/202 [13:03:37<53:20, 246.21s/it] The number of images classified correctly for class ----> forklift: 0  \n",
    "Macinando classi:  94% 190/202 [13:04:57<39:16, 196.35s/it] The number of images classified correctly for class ----> custard apple: 1  \n",
    "Macinando classi:  95% 191/202 [13:05:36<27:22, 149.35s/it] The number of images classified correctly for class ----> manhole cover: 1  \n",
    "Macinando classi:  95% 192/202 [13:09:14<28:18, 169.80s/it] The number of images classified correctly for class ----> goblet: 6  \n",
    "Macinando classi:  96% 193/202 [13:11:58<25:13, 168.19s/it] The number of images classified correctly for class ----> hummingbird: 38  \n",
    "Macinando classi:  96% 194/202 [13:22:38<41:17, 309.67s/it] The number of images classified correctly for class ----> apron: 0  \n",
    "Macinando classi:  97% 195/202 [13:24:48<29:50, 255.73s/it] The number of images classified correctly for class ----> racket: 1  \n",
    "Macinando classi:  97% 196/202 [13:27:08<22:05, 220.96s/it] The number of images classified correctly for class ----> accordion: 1  \n",
    "Macinando classi:  98% 197/202 [13:27:41<13:43, 164.66s/it] The number of images classified correctly for class ----> crayfish: 3  \n",
    "Macinando classi:  98% 198/202 [13:38:11<20:16, 304.22s/it] The number of images classified correctly for class ----> obelisk: 4  \n",
    "Macinando classi:  99% 199/202 [13:40:27<12:41, 253.72s/it] The number of images classified correctly for class ----> tank: 3  \n",
    "Macinando classi:  99% 200/202 [13:43:12<07:34, 227.02s/it] The number of images classified correctly for class ----> feather boa: 2  \n",
    "Macinando classi: 100% 201/202 [13:44:02<02:53, 173.99s/it] The number of images classified correctly for class ----> mosque: 1  \n",
    "Macinando classi: 100% 202/202 [13:45:31<00:00, 245.21s/it]    \n",
    "__________________________________________________________\n",
    "Total number of images: 7450  \n",
    "Number of correctly classified images: 1056  \n",
    "Accuracy: 0.141744966442953  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
